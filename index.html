<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Home</title>
    <!-- Load style.css -->
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <!-- Header with a background color filling approx. 300px and that has a title of the workshop and the date as a byline -->
    <header>
      <h1>NeurIPS Mechanistic Interpretability Workshop</h1>
      <p>December 11th, 2023</p>
    </header>
    <!-- Content on white background with sections Overview, Schedule, Speakers and Organizing Committee -->
    <main>
      <section>
        <h2>Overview</h2>
        <p>
          The workshop on Mechanistic Interpretability (MI) seeks to explore the
          dynamics of specific machine learning models and gain a better
          understanding of the mechanisms that determine their function. MI is
          more ambitious than classical Interpretability, in that the goal is to
          uncover and manipulate these mechanisms to refine machine learning
          models and for greater understanding of the functions and
          representations inside these models. Recent breakthroughs, such as the
          identification of the key- value-map nature of LM MLP layers, have
          opened up a world of opportunities for model editing and optimization.
          Research on Mechanistic Interpretability has grown in relevance with
          the emergence of large-scale language models and the use of
          jailbreaking and other creative techniques that demonstrate that
          previous ML techniques, such as RLHF, are unable to guarantee desired
          behaviors. The aim of Mechanistic Interpretability is to make ML
          models as reliable as traditionally coded software engineering and
          data structures, to enable the assembly of model building blocks with
          predictable results. Current work in this area includes reverse
          engineering transformers, model editing, and other work studying the
          inner workings of ML models. The aim of this workshop is to bring
          together experts and practitioners in the field of Mechanistic
          Interpretability to discuss and collaborate on their ideas, research
          and challenges. We hope to foster a rich dialogue about the challenges
          of developing principled approaches to understanding how these
          networks work and the challenges of deploying mechanistic
          interpretability in real-world applications. The workshop will involve
          talks from invited leading researchers and practitioners, as well as a
          panel discussion featuring experts in the field. We will hold talks
          from the pool of accepted papers as well as an open poster session,
          some encouraging discussion among attendees.
        </p>
        <p>
          Objectives The workshop's goals are: 1) To discuss the current state
          of Mechanistic Interpretability research, including recent advances,
          applications and challenges; 2) To identify areas in which further
          research is needed to develop methods for deriving meaningful insights
          from machine learning models; 3) To leverage the expertise of those in
          attendance to propose novel and innovative approaches to
          interpretability and promote research collaboration.
        </p>
      </section>
      <section>
        <h2>Schedule</h2>
        <p>Coming soon!</p>
      </section>
      <section>
        <h2>Speakers</h2>
        <div class="speakers">
          <div class="speaker">
            <img src="https://via.placeholder.com/150" alt="Speaker" />
            <div>
              <h3>Chris Olas</h3>
              <p>Anthropic</p>
            </div>
          </div>
          <div class="speaker">
            <img src="https://via.placeholder.com/150" alt="Speaker" />
            <div>
              <h3>David bau</h3>
              <p>Northeastern University</p>
            </div>
          </div>
          <div class="speaker">
            <img src="https://via.placeholder.com/150" alt="Speaker" />
            <div>
              <h3>Yonathan Belinkov</h3>
              <p>Technion IIT</p>
            </div>
          </div>
        </div>
      </section>
      <section>
        <h2>Organizing Committee</h2>
        <div class="speakers">
          <div class="speaker">
            <img src="https://via.placeholder.com/150" alt="Speaker" />
            <div>
              <h3>Organizer 1</h3>
              <p>Organizer 1 Affiliation</p>
            </div>
          </div>
          <div class="Organizer">
            <img src="https://via.placeholder.com/150" alt="Organizer" />
            <div>
              <h3>Organizer 2</h3>
              <p>Organizer 2 Affiliation</p>
            </div>
          </div>
          <div class="Organizer">
            <img src="https://via.placeholder.com/150" alt="Organizer" />
            <div>
              <h3>Organizer 3</h3>
              <p>Organizer 3 Affiliation</p>
            </div>
          </div>
        </div>
      </section>
    </main>
  </body>
</html>
